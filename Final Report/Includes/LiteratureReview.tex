\chapter{Literature Review}

\section{Introduction}
	Segmentation and classification of indoor scans is a reasonably new problem, a lot of the processes used in the newly created algorithms are not new. They are simply tweaked in such a way so as to produce the result required for an indoor environment. 
	
	This section is concerned with looking at new and old publications of the processes and algorithms associated with creating the program this thesis set out to make.
	
	It also looks at other papers and articles that have created similar programs and systems.
	
\section{Previous Work}

	\subsection{Commercially Available Software}
	
		The most common method of turning point clouds into 3D models is by having a person digitize the whole cloud with CAD software. This is a far from an efficient way of getting architecturally accurate models but is the most common because software that does it is not widely available. 
		% % % % % % % % % % % % % % % % %
		%Try find more or remove section
		% % % % % % % % % % % % % % % % %

	\subsection{Use of Primitives in Uncluttered Environments}
		\cite{sanchez_planar_2012} from Video and Image Processing Lab in the University of California, Berkeley, wrote a paper on planar 3D modeling of building interiors from point clouds generated by laser scanners. The scans that they used were cleaned out of noise and were not in cluttered environments.
		
		The method that \citeauthor{sanchez_planar_2012} proposed did not use any specific segmentation algorithm, they instead calculated the normals for each point and classified points based on the direction of their normal. From there they fit planes to represent walls. They also attempt to create stairs by looking at all the points labeled as 'remaining' and attempt to fit a predefined staircase model to the points.
		
		This idea could obviously be extended to any item where a primitive can be created. 
		
	\subsection{Occlusion in Cluttered Environments}
		In \citeyear{mura_automatic_2014} \citeauthor{mura_automatic_2014} Presented a robust approach for reconstructing the main architectural structure of complex indoor environments given laser scans of the rooms. within the paper they speak about how large vertical objects can be mistaken for a wall when they are in fact a cabinet or locker or something of that nature.
		
		Walls, by definition, cover the full vertical extent of a room. But when working wit laser scans this is not something that you can enforce because there is often clutter in the way so the full extent of the wall is not seen. So to deal with this \citeauthor{mura_automatic_2014} employed a simple visibility test to determine if one segment was 'in-front' of another. They cast rays from the scan center through the OBB (oriented bounding box) and set out to determine if those rays intersected any other segment. If they did the first segment obviously lies in-front of the other.
	
	\subsection{Segment Boundary Adjustment}
		[Not sure where to find information on the moving of segments corners to nearest line]

	
\section{Basis of Program}
	\subsection{Normal Determination}
		For a point to exist in a cloud and have x, y, z colour and intensity values associated with it, the point will have had to be on the surface of something. So with this knowledge we can start to create information about this point.
		
		\subsubsection{Principal Components Analysis}
			Principal Components Analysis (PCA), is a statistical technique that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called Principal components. the number of principal components is less than or equal to the dimensions of the data set, so in the case of point clouds 3.
			
			The first of the Principal components represents the largest variability in the data set, with the variability decreasing as you go along. essentially resulting in the 3rd Principal component being the normal to the surface \citep{dunteman_principal_1989}. 
		
		\subsubsection{Plane fitting}
			Another method for calculating normals is by least squares plane fitting. Least squares was created in 1805 by a French mathematician Adrien-Marie Legendre.
			
			Least squares is used to fit a plane to a point to get the normal of the point by taking a region around the point and fitting the plane to all those points. The process of fitting a plane to a set of points is described by \cite{schomaker_fit_1959}.
		
	
		
	\subsection{Segmentation}
		For automatic processing of point clouds the segmentation of the point cloud is one of the most important processes. it is important that the segmentation of the point cloud is correct and the method is the best for the use.
	
		\subsubsection{Edge-based Segmentation}
			Edge-based segmentation has two main sections to it; first to detect edges in the point cloud, and secondly to group all points contained within the edges as one segment.
			
			The algorithm used, however, does not use Edge-based techniques further reading on the topic can be found with \cite{sappa_fast_2001} and \cite{bhanu_automatic_1986}.
			
			
		\subsubsection{Region-based Segmentation}
			Region-based segmentation algorithms look for areas that fit into a certain criteria and group them together as a single region.
			
			Region growing for 3D point clouds is an adapted algorithm that was originally created by \cite{adams_seeded_1994} for the segmentation of intensity images.
			
			Region growing needs two things; first, seed points based on their curvature values, and secondly criteria in which to extend these points into regions.
			
			There are several methods for doing these two things as described by \cite{hoover_experimental_1996}.		
			
			\cite{rabbani_segmentation_2006} proposed the idea of adding a smoothness constraint which finds smoothly connected areas in the point cloud. The method they propose requires a small number of parameters which provide a trade off between over and under segmentation. It is this refined algorithm that point cloud library uses in as its default region growing.
			
			
	\subsection{Data Structures}
		In computer science trees are a widely used data structure or abstract data type. In the context of this paper they are used for creating a search-able data structure to make searching for nearest neighbors faster.
		
		\subsubsection{K-d tree}
			A K-d tree is a space partitioning data structure used for structuring unorganized points. a K-d tree is a structure designed specifically for multi dimensional situations, hence the name k-dimensional tree, where k is the dimension of the search space. The advantage of this data type is that is can handle many different types of queries very efficiently \citep{bentley_multidimensional_1975}.
			
			
			
	\subsection{Random sample consensus}
		Random sample consensus (RANSAC) is an iterative method used to estimate the model coefficients of a set of observed data that contains outliers. RANSAC was developed by \citeauthor{fischler_random_1981} in \citeyear{fischler_random_1981} as a way of solving the Location Determination Problem in photogrammetry and computer vision applications.
		
		RANSAC achieves its goal by iteratively selecting a random subset of the original data, this subset of the original data are considered inliers. A model is then fitted to the inliers (A plane or line ect). The remainder of the data set is then compared to the model if a point fits well with the model it is considered a hypothetical inlier. The model is then re-estimated from the hypothetical inliers. The model is then evaluated by the error relative to the inliers of the model.
		
		This process is repeated a fixed number of times, each time either rejecting the model if there are too few inliers, or replacing the last saved model if the error is lower.
		
	
	\subsection{Open Multi-Processing}
		OpenMP is an application programming interface (API) for writing multi-threaded applications. Certain functions in Point Cloud Library have support for multi-threading using OpenMP.
		
		OpenMP is used in some of Point Cloud Libraries stock functions and allow multi-threading to speed up processing times. unfortunately it is not supported for the segmentation algorithms because race conditions prevent it from being correctly implemented. 
		
		The Normal estimation class in Point Cloud Library has an OpenMP replacement that is 100\% compatible with the single-threaded function, so requires no effort to run the normal estimation up to 8 times faster depending on the number of cores the computer has to offer.